## import requests# import re# import os# import sys### def help(script):#     text = 'python3 %s 	https://pan.dldy.cf/collection/%E8%BD%AF%E4%BB%B6%E4%B8%8E%E6%95%99%E5%AD%A6/ ./' % script##     print(text)### def get_file(url, path):  ##文件下载函数#     content = requests.get(url)#     print("write %s in %s" % (url, path))#     filew = open(path + url.split("/")[-1], 'wb')#     for chunk in content.iter_content(chunk_size=512 * 1024):#         if chunk:  # filter out keep-alive new chunks#             filew.write(chunk)#     filew.close()### def get_dir(url, path):  # 文件夹处理逻辑#     content = requests.get(url).text#     if "mdui-list" in content:#         sub_url = re.findall('href="(.*?)"', content)##         print(sub_url)#         for i in sub_url:#             if "/" in i:##                 i = i.split("/")[0]#                 print(i)#                 if i != "." and i != "..":#                     if not os.direxists(path + i):#                         os.mkdir(path + i)##                     get_dir(url + "/" + i, path + i + "/")#                     print("url:" + url + "/" + i + "\nurl_path:" + path + i + "/")#             else:#                 get_file(url + "/" + i, path)#     else:#         get_file(url, path)### if __name__ == '__main__':#     # if len(sys.argv) <= 1:#     #     help(sys.argv[0])#     #     exit(0)#     # else:#     get_dir("https://pan.dldy.cf/collection/%E8%BD%AF%E4%BB%B6%E4%B8%8E%E6%95%99%E5%AD%A6/",#                 "/Users/wangbiao/github/my/PythonFile/TensorFlowDemo/downFile/file/")# import requests## file_url = "https://pan.dldy.cf/collection/短视频合集/校花洗澡被偷拍跟迷姦/1-sleep.mp4"## r = requests.get(file_url, stream=True)## with open("1-sleep.mp4", "wb") as pdf:#     for chunk in r.iter_content(chunk_size=1024):#         if chunk:#             pdf.write(chunk)import datetimeimport threadingimport timefrom urllib.request import quote, unquoteimport requestsimport tqdmimport jsonimport osimport aiohttpimport asynciofrom tqdm import tqdmimport requestsfrom bs4 import BeautifulSoup# archive_url = "http://www-personal.umich.edu/~csev/books/py4inf/media/"# headers = {#     ':authority': 'pan.dldy.cf',#     ':method': 'POST',#     ':path': '/collection/%E7%9F%AD%E8%A7%86%E9%A2%91%E5%90%88%E9%9B%86/',#    ':scheme':'https',#     'accept': '*/*',#    'accept-encoding': 'gzip, deflate, br',#     'accept-language': 'zh-CN,zh;q=0.9',#     'cache-control': 'no-cache',#     'content-length': '19',#     'content-type': 'application/x-www-form-urlencoded; charset=UTF-8',#     'cookie': '__cfduid=d62b38fcb09deb75d7a3e8b40cad8213b1590632850',#     'origin': 'https://pan.dldy.cf',#     'pragma': 'no-cache',#     'referer': 'https://pan.dldy.cf/collection/%E7%9F%AD%E8%A7%86%E9%A2%91%E5%90%88%E9%9B%86/',#     'sec-fetch-dest': 'empty',#     'sec-fetch-mode':'cors',#     'sec-fetch-site': 'same-origin',#     'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) '#                 'Chrome/81.0.4044.129 Safari/537.36',#     'x-requested-with': 'XMLHttpRequest'#     }# # headers = {# #     'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36',# #     'cookie': 'juzi_user=642230; juzi_token=bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczpcL1wvd3d3Lml0anV6aS5jb21cL2FwaVwvYXV0aG9yaXphdGlvbnMiLCJpYXQiOjE1ODU3MjczODIsImV4cCI6MTU4NTczMDk4MiwibmJmIjoxNTg1NzI3MzgyLCJqdGkiOiJuTlVpRkZTYmZKQm9sbER4Iiwic3ViIjo2NDIyMzAsInBydiI6IjIzYmQ1Yzg5NDlmNjAwYWRiMzllNzAxYzQwMDg3MmRiN2E1OTc2ZjciLCJ1dWlkIjoiWjlPekRpIn0.ulRlMMo16scpaE2cHdHes294aviMNUMDImmg2yn73dA',# #     'accept': 'application/json, text/plain, */*',# #     'accept-encoding': 'gzip, deflate, br',# #     'accept-language': 'zh-CN,zh;q=0.9',# #     'authorization': '"bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczpcL1wvd3d3Lml0anV6aS5jb21cL2FwaVwvYXV0aG9yaXphdGlvbnMiLCJpYXQiOjE1ODU3MjczODIsImV4cCI6MTU4NTczMDk4MiwibmJmIjoxNTg1NzI3MzgyLCJqdGkiOiJuTlVpRkZTYmZKQm9sbER4Iiwic3ViIjo2NDIyMzAsInBydiI6IjIzYmQ1Yzg5NDlmNjAwYWRiMzllNzAxYzQwMDg3MmRiN2E1OTc2ZjciLCJ1dWlkIjoiWjlPekRpIn0.ulRlMMo16scpaE2cHdHes294aviMNUMDImmg2yn73dA"',# #     'cache-control': 'no-cache',# #     'content-length': '21',# #     'content-type': 'application/json;charset=UTF-8',# #     'Cookie': 'juzi_user=642230; juzi_token=bearer '# #               'eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczpcL1wvd3d3Lml0anV6aS5jb21cL2FwaVwvYXV0aG9yaXphdGlvbnMiLCJpYXQiOjE1ODU3MjczODIsImV4cCI6MTU4NTczMDk4MiwibmJmIjoxNTg1NzI3MzgyLCJqdGkiOiJuTlVpRkZTYmZKQm9sbER4Iiwic3ViIjo2NDIyMzAsInBydiI6IjIzYmQ1Yzg5NDlmNjAwYWRiMzllNzAxYzQwMDg3MmRiN2E1OTc2ZjciLCJ1dWlkIjoiWjlPekRpIn0.ulRlMMo16scpaE2cHdHes294aviMNUMDImmg2yn73dA',# #     'curlopt_followlocation': 'true',# #     'origin': 'https://www.itjuzi.com',# #     'pragma': 'no-cache',# #     'referer': 'https://www.itjuzi.com/',# #     'sec-fetch-dest': 'empty',# #     'sec-fetch-mode': 'cors',# #     'sec-fetch-site': 'same-origin'# #     }# session = requests.Session()# response = session.get('https://www.itjuzi.com/investevent', headers=headers)def get_video_links(url, downPath):    # pathlist = []    # video_link = unquote(dirs[j], encoding='utf-8')    url = unquote(url, encoding='utf-8')    r = requests.post(url)    bodyJson = r.text    data2 = json.loads(bodyJson)    # array = json.loads()    for i in data2['files']:        # 添加文件目录        dirs = url.split('/')        tempMap = downPath;        # 用for循环来实现# 因为开头和结尾不是文件夹，因此从1开始，并排除数组的最后一项，也就是文件名： xx.jpg        for j in range(4, len(dirs) - 1):            if dirs[j] != None:                # 解码                tempMap = tempMap + "/" + dirs[j];                if not os.path.exists(tempMap):                    os.mkdir(tempMap)        print("目录创建完了")        if i['mimeType'] == 'application/vnd.google-apps.folder':            print("这个是个文件夹----" + i['name'])            print(url + i['name'] + "/")            res1 = quote(url + i['name'] + "/", safe=";/?:@&=+$,", encoding="utf-8")  # 编码            # if i['name'].find("91") == -1            get_video_links(res1, downPath)        else:            # pathlist.append(url + "/" + i['name'])            print("----文件------" + i['name'])            # print(url.encode('utf8').decode('unicode_escape'))            download_video_series(url + "/" + i['name'], tempMap)    # soup = BeautifulSoup(r.content, 'html5lib')    # links = soup.findAll('a')    # returnimport aiohttpimport asynciofrom tqdm import tqdmdef download_from_url(url, dst):    '''同步'''    response = requests.get(url, stream=True)    file_size = int(response.headers['content-length'])    if os.path.exists(dst):        first_byte = os.path.getsize(dst)    else:        first_byte = 0    if first_byte >= file_size:        return file_size    header = {"Range": f"bytes={first_byte}-{file_size}"}    pbar = tqdm(        total=file_size, initial=first_byte,        unit='B', unit_scale=True, desc=dst)    req = requests.get(url, headers=header, timeout=60, stream=True)    with(open(dst, 'ab')) as f:        for chunk in req.iter_content(chunk_size=1024):            if chunk:                f.write(chunk)                pbar.update(1024)    pbar.close()    return file_sizedef thread(url, filename):    r = requests.get(url, headers=None, stream=True, timeout=30)    # print(r.status_code, r.headers)    headers = {}    all_thread = 1    # 获取视频大小    file_size = int(r.headers['content-length'])    # 如果获取到文件大小，创建一个和需要下载文件一样大小的文件    if file_size:        fp = open(filename, 'wb')        fp.truncate(file_size)        print('视频大小：' + str(int(file_size / 1024 / 1024)) + "MB")        fp.close()    # 每个线程每次下载大小为5M    size = 5242880    # 当前文件大小需大于5M    if file_size > size:        # 获取总线程数        all_thread = int(file_size / size)        # 设最大线程数为10，如总线程数大于10        # 线程数为10        if all_thread > 10:            all_thread = 10    part = file_size // all_thread    threads = []    starttime = datetime.datetime.now().replace(microsecond=0)    for i in range(all_thread):        # 获取每个线程开始时的文件位置        start = part * i        # 获取每个文件结束位置        if i == all_thread - 1:            end = file_size        else:            end = start + part        if i > 0:            start += 1        headers = headers.copy()        headers['Range'] = "bytes=%s-%s" % (start, end)        t = threading.Thread(target=Handler, name='th-' + str(i),                             kwargs={'start': start, 'end': end, 'url': url, 'filename': filename, 'headers': headers})        t.setDaemon(True)        threads.append(t)    # 线程开始    for t in threads:        time.sleep(0.2)        t.start()    # 等待所有线程结束    for t in threads:        t.join()    endtime = datetime.datetime.now().replace(microsecond=0)    print('用时：%s' % (endtime - starttime))def Handler(start, end, url, filename, headers={}):    tt_name = threading.current_thread().getName()    print(tt_name + ' is begin')    r = requests.get(url, headers=headers, stream=True)    total_size = end - start    downsize = 0    startTime = time.time()    with open(filename, 'r+b') as fp:        fp.seek(start)        var = fp.tell()        for chunk in r.iter_content(204800):            if chunk:                fp.write(chunk)                downsize += len(chunk)                line = tt_name + '-downloading %d KB/s - %.2f MB， 共 %.2f MB'                line = line % (                    downsize / 1024 / (time.time() - startTime), downsize / 1024 / 1024,                    total_size / 1024 / 1024)                print(line, end='\r')def download_video_series(video_link, filePath):    # for link in video_links:    # dirs = video_link.split('/')    #    # # 用for循环来实现# 因为开头和结尾不是文件夹，因此从1开始，并排除数组的最后一项，也就是文件名： xx.jpg    # for i in range(4, len(dirs) - 1):    #    #     if dirs[i] != None:    #         #解码    #         tempMap = tempMap + "/" + dirs[i];    #         if not os.path.exists(tempMap):    #             os.mkdir(tempMap)    print("正在下载" + filePath)    file_name = video_link.split('/')[-1]    # async_download_from_url(video_link,dst)    # download_from_url(video_link, filePath+"/"+file_name)    # task = [asyncio.ensure_future(async_download_from_url(url, f"{i}.mp4")) for i in range(1, 12)]    thread(video_link, filePath + "/" + file_name)    # print("Downloading file:%s" % file_name)    # r = requests.get(video_link, stream=True)    #    # # download started    # with open(filePath +"/"+ file_name, 'wb') as f:    #     for chunk in r.iter_content(chunk_size=1024 * 1024):    #         if chunk:    #             f.write(chunk)    #    # print("%s downloaded!\n" % file_name)    # print("All videos downloaded!")    returnif __name__ == "__main__":    archive_url = ""    video_links = get_video_links(archive_url, '/Users/下载路径')    while True:        print("程序执行中。。。。。")        print("Start : %s" % time.ctime())        time.sleep(5)        print("End : %s" % time.ctime())    print("下载完成。。。。。")